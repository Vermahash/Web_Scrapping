{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d9ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "# ------------------ BART Summarizer Setup ------------------ #\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "summarizer = pipeline(\"summarization\", model=model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ------------------ NLTK Sentiment Analyzer Setup ------------------ #\n",
    "\"\"\"nltk.download('vader_lexicon')\"\"\"\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def summarize_with_bart(text):\n",
    "    try:\n",
    "        input_tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        input_len = input_tokens.shape[1]\n",
    "\n",
    "        if input_len < 10:\n",
    "            return text.strip()\n",
    "\n",
    "        max_len = min(100, input_len - 1)\n",
    "        min_len = max(5, max_len // 2)\n",
    "\n",
    "        summary = summarizer(\n",
    "            text,\n",
    "            max_length=max_len,\n",
    "            min_length=min_len,\n",
    "            do_sample=False\n",
    "        )\n",
    "        return summary[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è BART summarization failed: {e}\")\n",
    "        return \"Summary not available.\"\n",
    "\n",
    "# ------------------ Sentiment Function ------------------ #\n",
    "def analyze_sentiment(text):\n",
    "    score = sid.polarity_scores(text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# ------------------ Selenium Setup ------------------ #\n",
    "def setup_driver():\n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "    service = Service()\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "# ------------------ Reuters Scraper ------------------ #\n",
    "def scrape_reuters_bs(keyword):\n",
    "    url = f\"https://www.reuters.com/site-search/?query={keyword}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.reuters.com/\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Reuters HTTP error: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.select(\"div.search-results__item\")\n",
    "    results = []\n",
    "\n",
    "    for article in articles[:50]:\n",
    "        try:\n",
    "            title_tag = article.find(\"h3\")\n",
    "            summary_tag = article.find(\"p\")\n",
    "            link_tag = article.find(\"a\", href=True)\n",
    "            if not title_tag or not summary_tag:\n",
    "                continue\n",
    "            title = title_tag.get_text(strip=True)\n",
    "            summary = summary_tag.get_text(strip=True)\n",
    "            link = link_tag['href']\n",
    "            if link.startswith(\"/\"):\n",
    "                link = \"https://www.reuters.com\" + link\n",
    "            results.append({\"title\": title, \"link\": link, \"summary\": summary})\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped one Reuters article due to: {e}\")\n",
    "    return results\n",
    "\n",
    "# ------------------ NPR Scraper with Pagination ------------------ #\n",
    "def scrape_npr_selenium(keyword, max_articles):\n",
    "    driver = setup_driver()\n",
    "    results = []\n",
    "    page = 1\n",
    "\n",
    "    while len(results) < max_articles:\n",
    "        url = f\"https://www.npr.org/search?query={keyword}&page={page}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, \"article.item\")\n",
    "\n",
    "        for article in articles:\n",
    "            if len(results) >= max_articles:\n",
    "                break\n",
    "            try:\n",
    "                try:\n",
    "                    title_elem = article.find_element(By.CSS_SELECTOR, \"h2, h3\")\n",
    "                    title = title_elem.text.strip()\n",
    "                except:\n",
    "                    continue\n",
    "                try:\n",
    "                    summary = article.find_element(By.CLASS_NAME, \"teaser\").text.strip()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                links = article.find_elements(By.TAG_NAME, \"a\")\n",
    "                link = \"\"\n",
    "                for a in links:\n",
    "                    href = a.get_attribute(\"href\")\n",
    "                    if href and \"/\" in href and \"/202\" in href:  # check for full article URL pattern\n",
    "                        link = href\n",
    "                        break\n",
    "                if not link:\n",
    "                    link = links[0].get_attribute(\"href\") if links else \"#\"\n",
    "\n",
    "                results.append({\"title\": title, \"link\": link, \"summary\": summary})\n",
    "            except Exception as e:\n",
    "                print(f\"NPR: Skipped one article due to: {e}\")\n",
    "                continue\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    driver.quit()\n",
    "    return results\n",
    "\n",
    "# ------------------ Main Execution ------------------ #\n",
    "if __name__ == \"__main__\":\n",
    "    keyword = input(\"Enter a keyword to search: \").strip()\n",
    "    max_articles = int(input(\"How many total NPR articles would you like to scrape? \"))\n",
    "    reuters_data = scrape_reuters_bs(keyword)\n",
    "    npr_data = scrape_npr_selenium(keyword, max_articles)\n",
    "    all_articles = reuters_data + npr_data\n",
    "\n",
    "    print(f\"\\nüîé Articles scraped: Reuters = {len(reuters_data)}, NPR = {len(npr_data)}\")\n",
    "\n",
    "    enhanced_articles = []\n",
    "    for article in all_articles:\n",
    "        try:\n",
    "            bart_summary = summarize_with_bart(article['summary'])\n",
    "            sentiment = analyze_sentiment(article['summary'])\n",
    "            enhanced_articles.append({\n",
    "                'Title': article['title'],\n",
    "                'Summary': bart_summary,\n",
    "                'Sentiment': sentiment,\n",
    "                'Read More': bart_summary,\n",
    "                'Link': article['link']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to enhance article: {e}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\n‚úÖ Successfully enhanced: {len(enhanced_articles)} articles\")\n",
    "\n",
    "    print(\"\\nüîç Bullet Point Summary:\")\n",
    "    for idx, article in enumerate(enhanced_articles, start=1):\n",
    "        print(f\"{idx}. {article['Summary']} ({article['Sentiment']})\")\n",
    "\n",
    "    print(\"\\nüìä Sentiment Breakdown:\")\n",
    "    sentiment_counts = Counter([a['Sentiment'] for a in enhanced_articles])\n",
    "    total = sum(sentiment_counts.values())\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        print(f\"- {sentiment}: {count} ({(count/total)*100:.1f}%)\")\n",
    "\n",
    "    view_more = input(\"\\nEnter the number of the article you want to read more about (or press Enter to skip): \").strip()\n",
    "    if view_more.isdigit():\n",
    "        index = int(view_more) - 1\n",
    "        if 0 <= index < len(enhanced_articles):\n",
    "            selected = enhanced_articles[index]\n",
    "            print(f\"\\nüìù Title: {selected['Title']}\")\n",
    "            print(f\"üìö Full Summary: {selected['Read More']}\")\n",
    "            print(f\"üîó Link: {selected['Link']}\")\n",
    "        else:\n",
    "            print(\"‚ùå Invalid article number.\")\n",
    "    else:\n",
    "        print(\"‚è≠ Skipped reading more.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
